"""
evaluation_controller
=====================

This module defines the controller for evaluating the reflection
removal results on a set of image pairs.  It orchestrates loading
images, generating crate masks, computing highlight reduction and
coefficient of variation metrics, saving the results to a CSV file
and optionally producing simple visualisations of the metric
distributions.

The controller relies on :class:`app.model.evaluation_model.EvaluationModel`
for the actual metric computations and uses matplotlib for
visualisations.  The top–level function :func:`process_images`
accepts directories for the input images, output images, mask
folder (unused in evaluation but kept for API consistency), and
evaluation output.  It processes all images whose extensions
match a small set of common formats (png, jpg, jpeg, bmp, tiff).

"""

from __future__ import annotations

from dataclasses import dataclass, field
from pathlib import Path
from typing import List, Dict

import cv2
import numpy as np

import matplotlib
matplotlib.use("Agg")  # Use non-interactive backend
import matplotlib.pyplot as plt

from app.model.evaluation_model import EvaluationModel


SUPPORTED_EXTS = {".png", ".jpg", ".jpeg", ".bmp", ".tif", ".tiff"}


@dataclass
class EvaluationResult:
    """Container for evaluation metrics of a single image pair."""

    filename: str
    ratio_highlight: float
    reduction_highlight: float
    cov_in: float
    cov_out: float
    ratio_cov: float
    reduction_cov: float

    def to_dict(self) -> Dict[str, float]:
        return {
            "filename": self.filename,
            "ratio_highlight": self.ratio_highlight,
            "reduction_highlight": self.reduction_highlight,
            "cov_in": self.cov_in,
            "cov_out": self.cov_out,
            "ratio_cov": self.ratio_cov,
            "reduction_cov": self.reduction_cov,
        }


def process_images(
    images_dir: Path,
    mask_dir: Path,  # mask_dir is unused but kept for signature consistency
    result_dir: Path,
    evaluation_dir: Path,
    save_plots: bool = True,
    v_percentile: float = 98.0,
    s_threshold: int = 255,
    v_k: float = 1.0,
) -> List[EvaluationResult]:
    """Process all images and compute evaluation metrics.

    Parameters
    ----------
    images_dir : pathlib.Path
        Directory containing the original input images.
    mask_dir : pathlib.Path
        Directory containing the final masks (unused here but kept for
        API consistency).  Evaluation uses crate masks generated by
        :class:`BoxMaskModel` directly.
    result_dir : pathlib.Path
        Directory containing the reflection removed images.  Filenames
        must match those in ``images_dir`` (with optional suffixes).
    evaluation_dir : pathlib.Path
        Directory where the CSV and optional plots will be saved.
    save_plots : bool, optional
        Whether to generate and save distribution plots.  Defaults to
        ``True``.
    v_percentile : float, optional
        Percentile for highlight detection.  This value and ``v_k``
        jointly determine the adaptive brightness threshold used to
        identify specular highlights.  Defaults to 98.0.
    s_threshold : int, optional
        Saturation threshold for highlight detection.  A value of 255
        (default) disables the saturation filter so that only brightness is
        used.
    v_k : float, optional
        Number of standard deviations added to the mean of V values
        within the crate when computing the adaptive brightness
        threshold.  The final threshold is ``max(percentile_threshold,
        mean + v_k * std)``.  Defaults to 1.0.

    Returns
    -------
    list[EvaluationResult]
        List of evaluation results for each processed image.
    """
    # Ensure evaluation directory exists
    evaluation_dir.mkdir(parents=True, exist_ok=True)
    # Instantiate evaluation model
    # Instantiate evaluation model with dynamic threshold parameters
    evaluator = EvaluationModel(v_percentile=v_percentile, s_threshold=s_threshold, v_k=v_k)
    results: List[EvaluationResult] = []
    # Gather input images
    image_paths = [p for p in images_dir.iterdir() if p.suffix.lower() in SUPPORTED_EXTS]
    image_paths.sort()
    for img_path in image_paths:
        stem = img_path.stem
        # Determine corresponding reflection removed image
        # Assume reflection images are named '<stem>_reflection_removed.*'
        # Search for any matching file in result_dir
        found = None
        for ext in SUPPORTED_EXTS:
            candidate = result_dir / f"{stem}_reflection_removed{ext}"
            if candidate.exists():
                found = candidate
                break
        if found is None:
            # Skip if no corresponding output
            print(f"[WARNING] Skipping {img_path.name}: no matching reflection removed file.")
            continue
        # Load images
        img_in = cv2.imread(str(img_path), cv2.IMREAD_COLOR)
        img_out = cv2.imread(str(found), cv2.IMREAD_COLOR)
        if img_in is None or img_out is None:
            print(f"[WARNING] Could not read image pair for {stem}. Skipping.")
            continue
        # Generate crate mask using evaluator's box model
        crate_mask = evaluator.generate_crate_mask(img_in)
        # Compute metrics
        metrics = evaluator.evaluate(img_in, img_out, crate_mask)
        results.append(EvaluationResult(
            filename=img_path.name,
            ratio_highlight=metrics["ratio_highlight"],
            reduction_highlight=metrics["reduction_highlight"],
            cov_in=metrics["cov_in"],
            cov_out=metrics["cov_out"],
            ratio_cov=metrics["ratio_cov"],
            reduction_cov=metrics["reduction_cov"],
        ))
    # Save results to CSV
    if results:
        import csv
        csv_path = evaluation_dir / "evaluation_results.csv"
        with open(csv_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=list(results[0].to_dict().keys()))
            writer.writeheader()
            for r in results:
                writer.writerow(r.to_dict())
        print(f"Saved evaluation results to {csv_path}")
    else:
        print("No results to save.")
    # Generate plots if requested
    if save_plots and results:
        # Extract metric arrays, ignoring NaN values
        ratio_h_vals = np.array([r.ratio_highlight for r in results], dtype=np.float32)
        ratio_h_vals = ratio_h_vals[np.isfinite(ratio_h_vals)]
        ratio_cov_vals = np.array([r.ratio_cov for r in results], dtype=np.float32)
        ratio_cov_vals = ratio_cov_vals[np.isfinite(ratio_cov_vals)]
        # Plot histograms
        if ratio_h_vals.size > 0:
            plt.figure()
            plt.hist(ratio_h_vals, bins=20, alpha=0.75)
            plt.title("Histogram of highlight remaining ratio (R_out/R_in)")
            plt.xlabel("Highlight remaining ratio")
            plt.ylabel("Frequency")
            plt.grid(True)
            plt.tight_layout()
            plt.savefig(evaluation_dir / "hist_ratio_highlight.png")
            plt.close()
        if ratio_cov_vals.size > 0:
            plt.figure()
            plt.hist(ratio_cov_vals, bins=20, alpha=0.75)
            plt.title("Histogram of CoV ratio (CoV_out/CoV_in)")
            plt.xlabel("CoV ratio")
            plt.ylabel("Frequency")
            plt.grid(True)
            plt.tight_layout()
            plt.savefig(evaluation_dir / "hist_ratio_cov.png")
            plt.close()
        # Boxplot for both metrics
        combined = []
        labels = []
        if ratio_h_vals.size > 0:
            combined.append(ratio_h_vals)
            labels.append("Highlight ratio")
        if ratio_cov_vals.size > 0:
            combined.append(ratio_cov_vals)
            labels.append("CoV ratio")
        if combined:
            plt.figure()
            plt.boxplot(combined, labels=labels)
            plt.title("Boxplot of evaluation ratios")
            plt.grid(True, axis="y")
            plt.tight_layout()
            plt.savefig(evaluation_dir / "boxplot_metrics.png")
            plt.close()
        print(f"Saved plots to {evaluation_dir}")

        mean_HLR = np.mean([r.reduction_highlight for r in results])
        mean_CoV = np.mean([r.reduction_cov for r in results])
        mean_HLR_remain = np.mean([r.ratio_highlight for r in results])

        print("=== Simple Summary (Box 全体) ===")
        print(f"Percentage of Highlights Remaining (PHR): {mean_HLR_remain*100:.2f}%")
        print(f"Highlight Reduction (HLR): {mean_HLR*100:.2f}%")
        print(f"CoV Reduction: {mean_CoV*100:.2f}%")
    return results